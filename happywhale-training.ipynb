{"cells":[{"cell_type":"markdown","metadata":{},"source":["# Imports"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Use this for static type checking of notebook in a terminal\n","# pip install -U nbqa\n","# nbqa mypy your_notebook.ipynb"]},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","trusted":true},"outputs":[],"source":["!export CUDA_LAUNCH_BLOCKING=1 # for tracing issues\n","!pip install timm\n","!pip install torchinfo\n","!pip install --upgrade torchmetrics\n","\n","import numpy as np\n","import pandas as pd\n","import os\n","import gc\n","import random\n","import wandb\n","from typing import Callable\n","from typing import Dict\n","from typing import Optional\n","from pathlib import Path\n","from PIL import Image\n","from PIL import ImageFile\n","ImageFile.LOAD_TRUNCATED_IMAGES = True # fixes a weird issue\n","\n","from sklearn.preprocessing import LabelEncoder\n","import torch\n","from torch import nn\n","import torch.nn.functional as F\n","from torch.utils.data import DataLoader\n","from torch.utils.data import Dataset\n","import torchmetrics\n","from torchinfo import summary\n","import pytorch_lightning as pl\n","from pytorch_lightning.loggers import WandbLogger\n","from pytorch_lightning.callbacks.model_checkpoint import ModelCheckpoint\n","\n","import timm\n","from timm.data.transforms_factory import create_transform\n","from timm.optim import create_optimizer_v2"]},{"cell_type":"markdown","metadata":{},"source":["# Configuration"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-04-13T15:30:19.634779Z","iopub.status.busy":"2022-04-13T15:30:19.634492Z","iopub.status.idle":"2022-04-13T15:30:19.641635Z","shell.execute_reply":"2022-04-13T15:30:19.640763Z","shell.execute_reply.started":"2022-04-13T15:30:19.634746Z"},"trusted":true},"outputs":[],"source":["class CFG:\n","    SEED = 42\n","    \n","    # Dataset\n","    N_FOLDS = 5\n","    NUM_WORKERS = 6 # number of threads for dataloaders\n","    \n","    ### Model\n","    MODEL_NAME = \"tf_efficientnet_b4\"\n","    PRETRAINED=True\n","    IMAGE_SIZE = 380\n","    EMBEDDING_SIZE = 512\n","    DROPOUT=0.0\n","    \n","    # Arcface\n","    S = 10\n","    M = 0.1\n","    \n","    # Training\n","    OPTIMIZER=\"adamW\"\n","    MODEL_PATH=\"model.ckpt\" # file to store the model checkpoints\n","    BATCH_SIZE = 32 # Effective batch size will be BATCH_SIZE*ACCUMULATE_GRAD_BATCHES\n","    ACCUMULATE_GRAD_BATCHES = 1 # \"1\" means updates model after every batch\n","    NUM_EPOCHS = 30\n","    LR = 0.001\n","    WEIGHT_DECAY = 0.000001\n","\n","    DEBUG = False # smaller set if debugging"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-04-13T15:30:38.529699Z","iopub.status.busy":"2022-04-13T15:30:38.529104Z","iopub.status.idle":"2022-04-13T15:30:38.53951Z","shell.execute_reply":"2022-04-13T15:30:38.538706Z","shell.execute_reply.started":"2022-04-13T15:30:38.529661Z"},"trusted":true},"outputs":[],"source":["# Make everything deterministic - probably redundant here but I wanted to be sure\n","def fix_seed(seed):\n","    random.seed(seed)\n","    np.random.seed(seed)\n","    torch.manual_seed(seed)\n","    torch.cuda.manual_seed_all(seed)\n","    torch.backends.cudnn.deterministic = True\n","    pl.seed_everything(seed)\n","\n","fix_seed(CFG.SEED)"]},{"cell_type":"markdown","metadata":{},"source":["# Logging"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-04-13T15:30:25.830193Z","iopub.status.busy":"2022-04-13T15:30:25.829626Z","iopub.status.idle":"2022-04-13T15:30:37.090715Z","shell.execute_reply":"2022-04-13T15:30:37.090028Z","shell.execute_reply.started":"2022-04-13T15:30:25.830146Z"},"trusted":true},"outputs":[],"source":["# For using WandB to track training statistics\n","from kaggle_secrets import UserSecretsClient\n","\n","user_secrets = UserSecretsClient()\n","wandb_api = user_secrets.get_secret(\"wandb\") \n","wandb.login(key=wandb_api)"]},{"cell_type":"markdown","metadata":{},"source":["# Dataset"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-04-13T15:33:59.114923Z","iopub.status.busy":"2022-04-13T15:33:59.114654Z","iopub.status.idle":"2022-04-13T15:33:59.122778Z","shell.execute_reply":"2022-04-13T15:33:59.122173Z","shell.execute_reply.started":"2022-04-13T15:33:59.114894Z"},"trusted":true},"outputs":[],"source":["# Dataset PATHs\n","BASE_PATH = '../input/happywhale-enhanced-luke'\n","DATA_PATH = '../input/happy-whale-and-dolphin'\n","CHECKPOINTS_PATH = '../input/hwmodelcheckpoint/tf_efficientnet_b4_380.ckpt'\n","TRAIN_DIR = f\"{BASE_PATH}/train_images\"\n","TRAIN_DIR2 = f\"{BASE_PATH}/imgextra (1)\"\n","TEST_DIR = f\"{DATA_PATH}/test_images\"\n","TRAIN_CSV_PATH = f\"{BASE_PATH}/data.csv\"\n","TEST_CSV_PATH = f\"{BASE_PATH}/sample_submission.csv\"\n","\n","OUTPUT_DIR = '/kaggle/working'\n","TRAIN_CSV_OUTPUT_PATH = f\"{OUTPUT_DIR}/train.csv\"\n","TEST_CSV_OUTPUT_PATH = f\"{OUTPUT_DIR}/test.csv\"\n","ENCODER_CLASSES_PATH = f\"{OUTPUT_DIR}/encoder_classes.npy\"\n","CHECKPOINTS_DIR = f\"{OUTPUT_DIR}/checkpoints\"\n","SUBMISSION_CSV_PATH = f\"{OUTPUT_DIR}/submission.csv\""]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-04-13T15:30:48.082489Z","iopub.status.busy":"2022-04-13T15:30:48.082229Z","iopub.status.idle":"2022-04-13T15:30:48.086317Z","shell.execute_reply":"2022-04-13T15:30:48.085651Z","shell.execute_reply.started":"2022-04-13T15:30:48.082461Z"},"trusted":true},"outputs":[],"source":["def get_image_path(id: str, dir: Path) -> str:\n","    return f\"{dir}/{id}\""]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-04-13T15:30:50.390679Z","iopub.status.busy":"2022-04-13T15:30:50.390205Z","iopub.status.idle":"2022-04-13T15:31:12.855353Z","shell.execute_reply":"2022-04-13T15:31:12.854657Z","shell.execute_reply.started":"2022-04-13T15:30:50.390641Z"},"trusted":true},"outputs":[],"source":["train_df = pd.read_csv(TRAIN_CSV_PATH)\n","\n","N_CLASSES = len(train_df[\"individual_id\"].unique())\n","\n","train_df[\"image_path\"] = train_df[\"image\"].apply(get_image_path, dir=TRAIN_DIR)\n","\n","# Integer encoding for individuals ids\n","encoder = LabelEncoder()\n","train_df[\"individual_id\"] = encoder.fit_transform(train_df[\"individual_id\"])\n","\n","# Change path of some photos because I messed up when I created the dataset\n","src = '../input/happywhale-enhanced-luke/imgextra (1)'\n","for f in os.listdir(src):\n","    train_df.at[train_df['image']==f, 'image_path'] = f\"{src}/{f}\"\n","\n","np.save(ENCODER_CLASSES_PATH, encoder.classes_) # save label encoder for use in other notebooks\n","\n","train_df.to_csv(TRAIN_CSV_OUTPUT_PATH, index=False)\n","train_df.head()"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-04-13T15:31:21.297195Z","iopub.status.busy":"2022-04-13T15:31:21.296911Z","iopub.status.idle":"2022-04-13T15:31:21.495875Z","shell.execute_reply":"2022-04-13T15:31:21.495075Z","shell.execute_reply.started":"2022-04-13T15:31:21.297164Z"},"trusted":true},"outputs":[],"source":["# create test_df for making predictions\n","test_df = pd.read_csv('../input/happy-whale-and-dolphin/sample_submission.csv')\n","\n","test_df[\"image_path\"] = test_df[\"image\"].apply(get_image_path, dir=TEST_DIR)\n","test_df.drop(columns=[\"predictions\"], inplace=True)\n","\n","test_df[\"individual_id\"] = 0 # Dummy id\n","\n","test_df.to_csv(TEST_CSV_OUTPUT_PATH, index=False)\n","test_df.head()"]},{"cell_type":"markdown","metadata":{},"source":["# Dataset Classes"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-04-13T15:32:33.244419Z","iopub.status.busy":"2022-04-13T15:32:33.243943Z","iopub.status.idle":"2022-04-13T15:32:33.265472Z","shell.execute_reply":"2022-04-13T15:32:33.263346Z","shell.execute_reply.started":"2022-04-13T15:32:33.244377Z"},"trusted":true},"outputs":[],"source":["class HappyWhaleDataset(Dataset):\n","    \"\"\"\n","    Class to Make any dataset\n","    \"\"\"\n","    def __init__(self, df: pd.DataFrame, transform: Optional[Callable] = None):\n","        self.df = df\n","        self.transform = transform\n","\n","        self.image_names = self.df[\"image\"].values\n","        self.image_paths = self.df[\"image_path\"].values\n","        self.targets = self.df[\"individual_id\"].values\n","\n","    def __getitem__(self, index: int) -> Dict[str, torch.Tensor]:\n","        image_name = self.image_names[index]\n","\n","        image_path = self.image_paths[index]\n","\n","        image = Image.open(image_path)\n","    \n","        if self.transform:\n","            image = self.transform(image)\n","\n","        target = self.targets[index]\n","        target = torch.tensor(target, dtype=torch.long)\n","\n","        return {\"image_name\": image_name, \"image\": image, \"target\": target}\n","\n","    def __len__(self) -> int:\n","        return len(self.df)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-04-13T15:32:49.657084Z","iopub.status.busy":"2022-04-13T15:32:49.656824Z","iopub.status.idle":"2022-04-13T15:32:49.671284Z","shell.execute_reply":"2022-04-13T15:32:49.670628Z","shell.execute_reply.started":"2022-04-13T15:32:49.657048Z"},"trusted":true},"outputs":[],"source":["class LitDataModule(pl.LightningDataModule):\n","    \"\"\"\n","    Lightning data module for testing, validation, and testing\n","    \"\"\"\n","    def __init__(\n","        self,\n","        train_csv: str,\n","        test_csv: str,\n","        val_fold: float,\n","        image_size: int,\n","        batch_size: int,\n","        num_workers: int,\n","    ):\n","        super().__init__()\n","\n","        self.save_hyperparameters()\n","\n","        self.train_df = pd.read_csv(train_csv)\n","        self.test_df = pd.read_csv(test_csv)\n","        \n","        # for training, includes augmentation\n","        self.transform_train = create_transform(\n","            input_size=(self.hparams.image_size, self.hparams.image_size),\n","            crop_pct=1.0,\n","        )\n","        # for evaluation, deactivates augmentation\n","        self.transform_eval = create_transform(\n","            input_size=(self.hparams.image_size, self.hparams.image_size),\n","            crop_pct=1.0,\n","            is_training=False,\n","        )\n","        \n","    def setup(self):\n","        # Split train df using fold\n","        train_df = self.train_df[self.train_df.folds != self.hparams.val_fold].reset_index(drop=True)\n","        val_df = self.train_df[self.train_df.folds == self.hparams.val_fold].reset_index(drop=True)\n","\n","        self.train_dataset = HappyWhaleDataset(train_df, transform=self.transform_train)\n","        self.val_dataset = HappyWhaleDataset(val_df, transform=self.transform_eval)\n","        self.test_dataset = HappyWhaleDataset(self.test_df, transform=self.transform_eval)\n","\n","    def train_dataloader(self) -> DataLoader:\n","        return self._dataloader(self.train_dataset, train=True)\n","\n","    def val_dataloader(self) -> DataLoader:\n","        return self._dataloader(self.val_dataset)\n","\n","    def test_dataloader(self) -> DataLoader:\n","        return self._dataloader(self.test_dataset)\n","\n","    def _dataloader(self, dataset: HappyWhaleDataset, train: bool = False) -> DataLoader:\n","        if train == True:\n","            batch_size = self.hparams.batch_size\n","        else:\n","            batch_size = self.hparams.batch_size*2 # double batch size for eval since there's no training\n","\n","        return DataLoader(\n","            dataset,\n","            batch_size=batch_size,\n","            shuffle=train,\n","            num_workers=self.hparams.num_workers,\n","            pin_memory=True,\n","            drop_last=train,\n","        )"]},{"cell_type":"markdown","metadata":{},"source":["# Loss Function"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-04-13T15:32:54.285176Z","iopub.status.busy":"2022-04-13T15:32:54.284516Z","iopub.status.idle":"2022-04-13T15:32:54.291659Z","shell.execute_reply":"2022-04-13T15:32:54.290899Z","shell.execute_reply.started":"2022-04-13T15:32:54.285116Z"},"trusted":true},"outputs":[],"source":["class SoftMax(nn.Module):\n","    \"\"\"\n","    Softmax loss, just a linear layer\n","    \"\"\"\n","    def __init__(self, \n","        num_features: int,\n","        num_classes: int,\n","    ):\n","        super(SoftMax, self).__init__()\n","        \n","        self.num_features = num_features\n","        self.n_classes = num_classes\n","        self.W = nn.Parameter(torch.FloatTensor(num_classes, num_features))\n","        nn.init.xavier_uniform_(self.W)\n","\n","    def forward(self, input: torch.Tensor, label: torch.Tensor, device: str = \"cuda\") -> torch.Tensor:\n","        x=input\n","        W=self.W\n","\n","        logits = F.linear(x, W)\n","\n","        return logits"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-04-13T15:32:58.011576Z","iopub.status.busy":"2022-04-13T15:32:58.010865Z","iopub.status.idle":"2022-04-13T15:32:58.020999Z","shell.execute_reply":"2022-04-13T15:32:58.019913Z","shell.execute_reply.started":"2022-04-13T15:32:58.01153Z"},"trusted":true},"outputs":[],"source":["class ArcFace(nn.Module):\n","    \"\"\"\n","    ArcFace Loss\n","    \"\"\"\n","    def __init__(self, \n","        num_features: int,\n","        num_classes: int,\n","        s: float, \n","        m: float):\n","        super(ArcFace, self).__init__()\n","        \n","        self.num_features = num_features\n","        self.n_classes = num_classes\n","        self.s = s\n","        self.m = m\n","        self.W = nn.Parameter(torch.FloatTensor(num_classes, num_features))\n","        nn.init.xavier_uniform_(self.W)\n","\n","    def forward(self, input: torch.Tensor, label: torch.Tensor, device: str = \"cuda\") -> torch.Tensor:\n","        # normalize features\n","        x = F.normalize(input)\n","        # normalize weights\n","        W = F.normalize(self.W)\n","        # dot product\n","        logits = F.linear(x, W)\n","\n","        if label is None:\n","            return logits\n","        \n","        # add margin\n","        theta = torch.acos(torch.clamp(logits, -1.0 + 1e-7, 1.0 - 1e-7)) # truncate it because we don't need that high resolution\n","        target_logits = torch.cos(theta + self.m)\n","\n","        # convert to one-hot encoding\n","        one_hot = torch.zeros_like(logits)\n","        one_hot.scatter_(1, label.view(-1, 1).long(), 1)\n","        output = logits * (1 - one_hot) + target_logits * one_hot\n","        \n","        # feature re-scale\n","        output *= self.s\n","\n","        return output"]},{"cell_type":"markdown","metadata":{},"source":["# Setup Model"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# summary of model architecture\n","# model = timm.create_model(model_name='tf_efficientnet_b4')\n","# print(summary(model))"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-04-13T15:50:48.747261Z","iopub.status.busy":"2022-04-13T15:50:48.746951Z","iopub.status.idle":"2022-04-13T15:50:48.77047Z","shell.execute_reply":"2022-04-13T15:50:48.769711Z","shell.execute_reply.started":"2022-04-13T15:50:48.747219Z"},"trusted":true},"outputs":[],"source":["class LitModule(pl.LightningModule):\n","    \"\"\"\n","    Lightning module\n","    \"\"\"\n","    def __init__(\n","        self,\n","        model_name: str,\n","        pretrained: bool,\n","        drop_rate: float,\n","        embedding_size: int,\n","        num_classes: int,\n","        arc_s: float,\n","        arc_m: float,\n","        optimizer: str,\n","        learning_rate: float,\n","        weight_decay: float,\n","        len_train_dl: int,\n","        epochs:int\n","    ):\n","        super().__init__()\n","\n","        self.save_hyperparameters()\n","\n","        # get pretrained model from timm\n","        self.model = timm.create_model(model_name, pretrained=pretrained, drop_rate=drop_rate)\n","        \n","        # embedding layer to take output of feature extractor\n","        self.embedding = nn.Linear(self.model.get_classifier().in_features, embedding_size)\n","        # get rid of classifier in timm model\n","        self.model.reset_classifier(num_classes=0, global_pool=\"avg\")\n","\n","        # create ArcFace\n","        self.arc = ArcFace(\n","            num_features=embedding_size,\n","            num_classes=num_classes,\n","            s=arc_s,\n","            m=arc_m,\n","        )\n","\n","        # use this if doing SoftMax\n","#         self.soft = SoftMax(\n","#             num_features=embedding_size,\n","#             num_classes=num_classes,\n","#         )\n","\n","        # create loss functions\n","        self.loss_fn = F.cross_entropy\n","        self.train_acc = torchmetrics.Accuracy()\n","        self.train_top_5_acc = torchmetrics.Accuracy(top_k=5)\n","        self.train_f1 = torchmetrics.F1Score(num_classes=num_classes)\n","        self.val_acc = torchmetrics.Accuracy()\n","        self.val_top_5_acc = torchmetrics.Accuracy(top_k=5)\n","        self.val_f1 = torchmetrics.F1Score(num_classes=num_classes)\n","\n","    def forward(self, images: torch.Tensor) -> torch.Tensor:\n","        features = self.model(images)\n","        embeddings = self.embedding(features)\n","\n","        return embeddings\n","    \n","    def configure_optimizers(self):\n","        # create optimizer using timm\n","        optimizer = create_optimizer_v2(\n","            self.parameters(),\n","            opt=self.hparams.optimizer,\n","            lr=self.hparams.learning_rate,\n","            weight_decay=self.hparams.weight_decay,\n","        )\n","        \n","        # create learning rate scheduler\n","        scheduler = torch.optim.lr_scheduler.OneCycleLR(\n","            optimizer,\n","            self.hparams.learning_rate,\n","            steps_per_epoch=self.hparams.len_train_dl,\n","            epochs=self.hparams.epochs,\n","        )\n","        scheduler = {\"scheduler\": scheduler, \"interval\": \"step\"}\n","\n","        return [optimizer], [scheduler]\n","\n","    def training_step(self, batch: Dict[str, torch.Tensor], batch_idx: int) -> torch.Tensor:\n","        images, targets = batch[\"image\"], batch[\"target\"]\n","\n","        embeddings = self(images)\n","        outputs = self.arc(embeddings, targets, self.device)\n","\n","        loss = self.loss_fn(outputs, targets)\n","        self.train_acc(outputs, targets)\n","        self.train_top_5_acc(outputs, targets)\n","        self.train_f1(outputs, targets)\n","        \n","        self.log(f\"train_loss\", loss, batch_size=CFG.BATCH_SIZE)\n","        self.log(f\"train_acc\", self.train_acc, batch_size=CFG.BATCH_SIZE)\n","        self.log(f\"train_top_5_acc\", self.train_top_5_acc, batch_size=CFG.BATCH_SIZE)\n","        self.log(f\"train_f1\", self.train_f1,  batch_size=CFG.BATCH_SIZE)\n","        return loss\n","\n","    def validation_step(self, batch: Dict[str, torch.Tensor], batch_idx: int) -> torch.Tensor:\n","        images, targets = batch[\"image\"], batch[\"target\"]\n","\n","        embeddings = self(images)\n","        outputs = self.arc(embeddings, targets, self.device)\n","\n","        loss = self.loss_fn(outputs, targets)\n","        self.val_acc(outputs, targets)\n","        self.val_top_5_acc(outputs, targets)\n","        self.val_f1(outputs, targets)\n","\n","        self.log(f\"val_loss\", loss, batch_size=CFG.BATCH_SIZE)\n","        self.log(f\"val_acc\", self.val_acc, batch_size=CFG.BATCH_SIZE)\n","        self.log(f\"val_top_5_acc\", self.val_top_5_acc, batch_size=CFG.BATCH_SIZE)\n","        self.log(f\"val_f1\", self.val_f1, batch_size=CFG.BATCH_SIZE)\n","\n","        return loss\n","    \n","    def predict_step(self, batch: Dict[str, torch.Tensor], batch_idx: int) -> torch.Tensor:\n","        images = batch[\"image\"]\n","        \n","        embeddings = self(images) # when there's no labels, it just does SoftMax with normalization\n","        pred = self.arc(embeddings, label=None, device=self.device)\n","        \n","        predtop = torch.topk(pred, 5) # saves values and indices of top5 predictions\n","        \n","        return predtop"]},{"cell_type":"markdown","metadata":{},"source":["# Setup Training"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-04-13T15:34:35.303621Z","iopub.status.busy":"2022-04-13T15:34:35.303361Z","iopub.status.idle":"2022-04-13T15:34:35.318804Z","shell.execute_reply":"2022-04-13T15:34:35.317923Z","shell.execute_reply.started":"2022-04-13T15:34:35.303592Z"},"trusted":true},"outputs":[],"source":["def train(\n","    train_csv: str = str(TRAIN_CSV_OUTPUT_PATH),\n","    test_csv: str = str(TEST_CSV_OUTPUT_PATH),\n","    val_fold: float = 2.0,\n","    image_size: int = 380,\n","    batch_size: int = 32,\n","    num_workers: int = 4,\n","    model_name: str = \"tf_efficientnet_b4\",\n","    log_name: str = \"EffnetA\",\n","    pretrained: bool = True,\n","    drop_rate: float = 0.0,\n","    embedding_size: int = 512,\n","    num_classes: int = 15587,\n","    arc_s: float = 30.0,\n","    arc_m: float = 0.5,\n","    optimizer: str = \"adam\",\n","    learning_rate: float = 3e-4,\n","    weight_decay: float = 1e-6,\n","    checkpoints_dir: str = str(CHECKPOINTS_DIR),\n","    accumulate_grad_batches: int = 1,\n","    auto_lr_find: bool = False,\n","    auto_scale_batch_size: bool = False,\n","    fast_dev_run: bool = False,\n","    gpus: int = 1,\n","    max_epochs: int = 10,\n","    precision: int = 16,\n","    stochastic_weight_avg: bool = True,\n","    continued_training: bool = False,\n","):\n","    \"\"\"\n","    Setup and run training\n","    \"\"\"\n","    \n","    torch.cuda.empty_cache()\n","    gc.collect()\n","    \n","    torch.autograd.set_detect_anomaly(False)\n","    torch.autograd.profiler.profile(False)\n","    torch.autograd.profiler.emit_nvtx(False)\n","    \n","    wandb_logger = WandbLogger(project=\"HappyWhale\", name=log_name)\n","    \n","    datamodule = LitDataModule(\n","        train_csv=train_csv,\n","        test_csv=test_csv,\n","        val_fold=val_fold,\n","        image_size=image_size,\n","        batch_size=batch_size,\n","        num_workers=num_workers,\n","    )\n","    \n","    datamodule.setup()\n","    len_train_dl = len(datamodule.train_dataloader())\n","\n","    if continued_training:\n","        module = LitModule.load_from_checkpoint(CHECKPOINTS_PATH)\n","    else:\n","        module = LitModule(\n","            model_name=model_name,\n","            pretrained=pretrained,\n","            drop_rate=drop_rate,\n","            embedding_size=embedding_size,\n","            num_classes=num_classes,\n","            arc_s=arc_s,\n","            arc_m=arc_m,\n","            optimizer=optimizer,\n","            learning_rate=learning_rate,\n","            weight_decay=weight_decay,\n","            len_train_dl=len_train_dl,\n","            epochs=max_epochs\n","        )\n","    \n","    model_checkpoint = ModelCheckpoint(\n","        checkpoints_dir,\n","        filename=f\"{model_name}_{image_size}\",\n","        monitor=\"train_loss\",\n","    )\n","        \n","    trainer = pl.Trainer(\n","        accumulate_grad_batches=accumulate_grad_batches,\n","        auto_lr_find=auto_lr_find,\n","        auto_scale_batch_size=auto_scale_batch_size,\n","        benchmark=True,\n","        callbacks=[model_checkpoint],\n","        deterministic=True,\n","        fast_dev_run=fast_dev_run,\n","        gpus=gpus,\n","        max_epochs=2 if CFG.DEBUG else max_epochs,\n","        precision=precision,\n","        stochastic_weight_avg=stochastic_weight_avg,\n","        limit_train_batches=0.1 if CFG.DEBUG else 1.0,\n","        limit_val_batches=0.1 if CFG.DEBUG else 1.0,\n","        logger=wandb_logger,\n","    )\n","\n","    trainer.tune(module, datamodule=datamodule)\n","\n","    # actual fitting\n","    trainer.fit(module, datamodule=datamodule)\n","    wandb_logger.finalize(\"success\")\n","    \n","    torch.cuda.empty_cache()\n","    gc.collect()"]},{"cell_type":"markdown","metadata":{},"source":["# Run Training"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-04-13T15:34:45.206326Z","iopub.status.busy":"2022-04-13T15:34:45.205382Z","iopub.status.idle":"2022-04-13T15:34:45.216519Z","shell.execute_reply":"2022-04-13T15:34:45.215724Z","shell.execute_reply.started":"2022-04-13T15:34:45.206289Z"},"trusted":true},"outputs":[],"source":["# run training with desired hparams\n","train(model_name=CFG.MODEL_NAME,\n","      image_size=CFG.IMAGE_SIZE,\n","      batch_size=CFG.BATCH_SIZE,\n","      arc_s=CFG.S,\n","      arc_m=CFG.M,\n","      num_classes=N_CLASSES,\n","      learning_rate=CFG.LR,\n","      optimizer=CFG.OPTIMIZER,\n","      max_epochs=CFG.NUM_EPOCHS,\n","      pretrained=CFG.PRETRAINED,\n","      drop_rate=CFG.DROPOUT,\n","      log_name=\"Train-CE-LR001-S10-M1-Adam\",\n","     )\n","\n","wandb.finish()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# download the checkpoints if it's a live notebook\n","# from shutil import make_archive\n","# directory = \"./checkpoints\"\n","# make_archive('chkpnts', 'zip', directory)\n","\n","# from IPython.display import FileLink\n","# FileLink(r'chkpnts.zip')"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.4"}},"nbformat":4,"nbformat_minor":4}
