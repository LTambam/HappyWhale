{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_kg_hide-input":true,"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.execute_input":"2022-04-11T22:22:43.964449Z","iopub.status.busy":"2022-04-11T22:22:43.964171Z","iopub.status.idle":"2022-04-11T22:22:46.599919Z","shell.execute_reply":"2022-04-11T22:22:46.598976Z","shell.execute_reply.started":"2022-04-11T22:22:43.96442Z"},"trusted":true},"outputs":[],"source":["import numpy as np\n","import pandas as pd\n","import os\n","import cv2\n","import matplotlib.pyplot as plt\n","import time\n","import glob\n","import shutil\n","import json\n","import random\n","import datetime\n","from tqdm.notebook import tqdm\n","from sklearn.model_selection import StratifiedKFold\n","import albumentations as A\n","np.random.seed(42)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-04-11T22:22:41.446649Z","iopub.status.busy":"2022-04-11T22:22:41.445924Z","iopub.status.idle":"2022-04-11T22:22:41.478097Z","shell.execute_reply":"2022-04-11T22:22:41.476925Z","shell.execute_reply.started":"2022-04-11T22:22:41.44647Z"},"trusted":true},"outputs":[],"source":["# max image size for EfficientNet-B4\n","MAX_IMAGE_SIZE = 380\n","\n","# only use 10 % of dataset for hparam tuning\n","HPARAM_SET = False\n","H_FOLDS = 10 # Number of folds\n","FOLDS = 3 # number of folds for training set (not actually 33% of dataset, only ids with > 3 images)\n","NUM_PICTURES = 5000\n","\n","# Output the dataset as a kaggle dataset (higher memory limit)\n","KAGGLE_DATASET_NAME = 'happywhale-luke' # Name of the resulting dataset\n","\n","USE_TRANSFORMATIONS = False\n","\n","TEST_MODE = False # Used for debugging"]},{"cell_type":"markdown","metadata":{},"source":["# Create a Kaggle Dataset"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-04-11T22:25:38.063276Z","iopub.status.busy":"2022-04-11T22:25:38.062825Z","iopub.status.idle":"2022-04-11T22:25:39.152401Z","shell.execute_reply":"2022-04-11T22:25:39.151208Z","shell.execute_reply.started":"2022-04-11T22:25:38.063243Z"},"trusted":true},"outputs":[],"source":["# authenticate Kaggle user to create a dataset directlyt from notebook\n","from kaggle_secrets import UserSecretsClient\n","user_secrets = UserSecretsClient()\n","apitoken = user_secrets.get_secret(\"kaggle\")\n","!mkdir -p ~/.kaggle\n","%store apitoken >~/.kaggle/kaggle.json"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-04-11T22:25:58.126803Z","iopub.status.busy":"2022-04-11T22:25:58.125797Z","iopub.status.idle":"2022-04-11T22:26:09.234829Z","shell.execute_reply":"2022-04-11T22:26:09.233669Z","shell.execute_reply.started":"2022-04-11T22:25:58.126744Z"},"trusted":true},"outputs":[],"source":["# !rm -r /kaggle/working/tmp/{KAGGLE_DATASET_NAME}\n","\n","# copy everything over to output folder\n","!cp ../input/happy-whale-and-dolphin/sample_submission.csv sample_submission.csv\n","!cp ../input/happy-whale-and-dolphin/train.csv train.csv\n","\n","# found this code on Kaggle to create dataset in a notebook\n","BASE_PATH = f\"/kaggle/working/tmp/{KAGGLE_DATASET_NAME}\"\n","!mkdir -p BASE_PATH\n","\n","with open('/root/.kaggle/kaggle.json') as f:\n","    kaggle_creds = json.load(f)\n","\n","os.environ['KAGGLE_USERNAME'] = kaggle_creds['username']\n","os.environ['KAGGLE_KEY'] = kaggle_creds['key']\n","\n","!kaggle datasets init -p /kaggle/working/tmp/{KAGGLE_DATASET_NAME}\n","\n","with open(f'/kaggle/working/tmp/{KAGGLE_DATASET_NAME}/dataset-metadata.json') as f:\n","    dataset_meta = json.load(f)\n","dataset_meta['id'] = f'luketambakis/{KAGGLE_DATASET_NAME}'\n","dataset_meta['title'] = KAGGLE_DATASET_NAME\n","with open(f'/kaggle/working/tmp/{KAGGLE_DATASET_NAME}/dataset-metadata.json', \"w\") as outfile:\n","    json.dump(dataset_meta, outfile)\n","print(dataset_meta)\n","\n","!cp /kaggle/working/tmp/{KAGGLE_DATASET_NAME}/dataset-metadata.json /kaggle/working/tmp/{KAGGLE_DATASET_NAME}/meta.json\n","!ls /kaggle/working/tmp/{KAGGLE_DATASET_NAME}\n","\n","!kaggle datasets create -u -p /kaggle/working/tmp/{KAGGLE_DATASET_NAME} \n","\n","# make directories for images\n","!mkdir {BASE_PATH}/test_images\n","!mkdir {BASE_PATH}/train_images\n","\n","INPUT_PATH = \"../input/happy-whale-and-dolphin/train_images\"\n","DIR_NAME = \"train_images\"\n","\n","!cp train.csv f\"{BASE_PATH}/train.csv\""]},{"cell_type":"markdown","metadata":{},"source":["# Pre-Process Train CSV"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-04-11T20:45:45.515115Z","iopub.status.busy":"2022-04-11T20:45:45.514588Z","iopub.status.idle":"2022-04-11T20:45:45.76275Z","shell.execute_reply":"2022-04-11T20:45:45.761805Z","shell.execute_reply.started":"2022-04-11T20:45:45.515063Z"},"trusted":true},"outputs":[],"source":["train = pd.read_csv(\"./train.csv\")\n","\n","train[\"species\"][train[\"species\"] == \"bottlenose_dolpin\"] = \"bottlenose_dolphin\"\n","train[\"species\"][train[\"species\"] == \"kiler_whale\"] = \"killer_whale\"\n","train[\"species\"][train[\"species\"] == \"globis\"] = \"short_finned_pilot_whale\"\n","train[\"species\"][train[\"species\"] == \"pilot_whale\"] = \"short_finned_pilot_whale\"\n","\n","# Adjust typos in \"species\" column\n","train[\"species\"] = train[\"species\"].replace([\"bottlenose_dolpin\", \"kiler_whale\", \"globis\", \"pilot_whale\"],\n","                                            [\"bottlenose_dolphin\",\"killer_whale\", \"short_finned_pilot_whale\", \"short_finned_pilot_whale\"])\n","\n","# Add attribute for presence of a dorsal fin\n","train[\"finned\"] = train[\"species\"].apply(lambda x: 'finless' if ((x=='beluga')or(x=='gray_whale')or(x=='southern_right_whale')) else 'finned')\n","\n","# Add attribute for suborder baleen and toothed\n","train[\"suborder\"] = train[\"species\"].apply(lambda x: 'baleen' if ((x=='humpback_whale')or(x=='minke_whale')or(x=='fin_whale')or(x=='blue_whale')or\\\n","                                                                  (x=='gray_whale')or(x=='southern_right_whale')or(x=='sei_whale')or(x=='brydes_whale')) else 'toothed')\n","\n","# Add attribute for family monodontidae, balaenidae, balaenopteridae, and delphinidae\n","train[\"family\"] = train[\"species\"].apply(lambda x: 'monodontidae' if (x=='beluga') else ('balaenidae' if (x=='southern_right_whale') else \\\n","                                                                                         ('balaenopteridae' if (x=='humpback_whale')or(x=='minke_whale')or\\\n","                                                                                         (x=='fin_whale')or(x=='blue_whale')or(x=='gray_whale')or(x=='sei_whale')or\\\n","                                                                                         (x=='brydes_whale') else 'delphinidae')))\n","\n","# remove bad photo - has a clipboard in it, no whale\n","train.drop(train.index[train['image'] == 'cd5fe465c60cb9.jpg'], inplace=True)\n","train.reset_index(inplace=True)\n","train.drop('index', axis=1, inplace=True)"]},{"cell_type":"markdown","metadata":{},"source":["# Split Data"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-04-11T16:53:41.932907Z","iopub.status.busy":"2022-04-11T16:53:41.931998Z","iopub.status.idle":"2022-04-11T16:53:42.161044Z","shell.execute_reply":"2022-04-11T16:53:42.160417Z","shell.execute_reply.started":"2022-04-11T16:53:41.932852Z"},"trusted":true},"outputs":[],"source":["if HPARAM_SET:\n","    # split into small dataset for hyperparameter tuning\n","    skf = StratifiedKFold(n_splits=10, random_state=42, shuffle=True)\n","    folds = np.zeros(len(train), dtype=np.uint8)\n","    for fold, ( _, val_) in enumerate(skf.split(X=train, y=train.individual_id)):\n","        folds[val_] = fold\n","    train[\"folds\"] = folds\n","    train2 = train[train['folds']==0]\n","\n","else:\n","    # get counts for each id\n","    counts = train.groupby(['individual_id']).size().reset_index(name='counts')\n","    counts = counts[counts['counts'] > 2]\n","    \n","    # get ids with at least 3 examples\n","    df_temp = train[train['individual_id'].isin(counts['individual_id'])]\n","\n","    # split temp into 3 stratified folds, so at least 1 example in each fold\n","    skf = StratifiedKFold(n_splits=3, random_state=42, shuffle=True)\n","    folds = np.zeros(len(df_temp), dtype=np.uint8)\n","    for fold, ( _, val_) in enumerate(skf.split(X=df_temp, y=df_temp.individual_id)):\n","        folds[val_] = fold\n","    df_temp[\"folds\"] = folds\n","\n","    # combine with full set - rest of the training set is fold 0, so 2 or 3 could be used for validation\n","    train[\"folds\"] = 0\n","    train.update(df_temp)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-04-11T16:53:58.048511Z","iopub.status.busy":"2022-04-11T16:53:58.048025Z","iopub.status.idle":"2022-04-11T16:53:58.053938Z","shell.execute_reply":"2022-04-11T16:53:58.053401Z","shell.execute_reply.started":"2022-04-11T16:53:58.048464Z"},"trusted":true},"outputs":[],"source":["transformations = A.Compose([\n","                A.HorizontalFlip(p=0.5),\n","                A.augmentations.transforms.HueSaturationValue(p=0.5),\n","                A.RandomBrightnessContrast(brightness_limit=0.10, contrast_limit=(-0.2, 0.2),p=0.5),\n","                A.Resize(MAX_IMAGE_SIZE, MAX_IMAGE_SIZE, cv2.INTER_CUBIC)\n","            ])"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-04-11T16:54:00.396788Z","iopub.status.busy":"2022-04-11T16:54:00.396297Z","iopub.status.idle":"2022-04-11T16:54:00.405955Z","shell.execute_reply":"2022-04-11T16:54:00.405109Z","shell.execute_reply.started":"2022-04-11T16:54:00.396742Z"},"trusted":true},"outputs":[],"source":["# augment images - need 2 examples for each id in hparam subset\n","if HPARAM_SET:\n","    # get counts of each ID in subset\n","    counts = train2.groupby(['individual_id']).size().reset_index(name='counts')\n","    for index, rows in tqdm(counts.iterrows(), total=counts.shape[0]):\n","\n","        tmp_df = train2[train2['individual_id']==rows['individual_id']]\n","        \n","        for i in range(2-rows['counts']):\n","            # pick random image from group of same id\n","            idx = random.choice(range(0, rows['counts']))\n","            path = tmp_df['image'].values[idx]\n","            image = cv2.imread(f\"{INPUT_PATH}/{path}\")\n","            image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n","            \n","            # augment image\n","            tmp_img = transformations(image=image)[\"image\"]\n","            \n","            # come up with new image name\n","            f = f\"{rows['individual_id']}_{str(i)}.jpg\"\n","            \n","            # write to dataset in bmp format\n","            output_path = os.path.join(BASE_PATH, DIR_NAME, f.split('.')[0] + \".bmp\")\n","            cv2.imwrite(output_path, tmp_img)\n","            \n","            # append new entry to dataframe\n","            output_df = tmp_df.iloc[[0]]\n","            output_df['image'] = f\n","            train2 = train2.append(output_df, ignore_index = True)\n","            \n","    train2[\"image\"] = train2[\"image\"].str[:-3] + \"bmp\"\n","    train2.to_csv(os.path.join(BASE_PATH, \"./train.csv\"), index=False)\n","\n","else: \n","    train[\"image\"] = train[\"image\"].str[:-3] + \"bmp\"\n","    train.to_csv(os.path.join(BASE_PATH, \"./train.csv\"), index=False)\n"]},{"cell_type":"markdown","metadata":{},"source":["# Alter Images and Output to Dataset"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-04-11T16:54:41.853177Z","iopub.status.busy":"2022-04-11T16:54:41.85288Z","iopub.status.idle":"2022-04-11T16:54:41.861149Z","shell.execute_reply":"2022-04-11T16:54:41.860005Z","shell.execute_reply.started":"2022-04-11T16:54:41.853142Z"},"trusted":true},"outputs":[],"source":["def copy_dir():\n","    # copy images to output directory\n","    \n","    path = os.path.join(INPUT_PATH, DIR_NAME)\n","    n = len(train)\n","\n","    for _, rows in tqdm(train.iterrows(), total=n):\n","        f = rows['image']\n","        \n","        image_path = os.path.join(path, f.split('.')[0] + \".jpg\")\n","        image = cv2.imread(image_path)\n","        \n","        if image.shape[0] > MAX_IMAGE_SIZE or image.shape[1] > MAX_IMAGE_SIZE:\n","            image = cv2.resize(image, (MAX_IMAGE_SIZE, MAX_IMAGE_SIZE), interpolation=cv2.INTER_CUBIC)\n","         \n","        new_path = os.path.join(BASE_PATH, DIR_NAME, f.split('.')[0] + \".bmp\") # output as bmp\n","            \n","        cv2.imwrite(new_path, image)\n","\n","copy_dir()"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2022-04-11T22:44:43.54772Z","iopub.status.idle":"2022-04-11T22:44:43.548292Z","shell.execute_reply":"2022-04-11T22:44:43.548124Z","shell.execute_reply.started":"2022-04-11T22:44:43.548103Z"},"trusted":true},"outputs":[],"source":["# Add photos to dataset and change version\n","!ls /kaggle/working/tmp/{KAGGLE_DATASET_NAME}\n","version_name = datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n","!kaggle datasets version -m {version_name} -p /kaggle/working/tmp/{KAGGLE_DATASET_NAME} -r zip -q"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.4"}},"nbformat":4,"nbformat_minor":4}
